{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7447aff9",
   "metadata": {},
   "source": [
    "\n",
    "# Anime-Llasa-3B — HF Space準拠 /tmp完結ノートブック（**v6：EOS即終了対策版**）\n",
    "- HF Space相当（ASR→参照プレフィックス→生成→XCodec2復号）\n",
    "- **/tmpのみ**使用、**Agg**固定、**torch 2.4.1+cu121**\n",
    "- **hf_transfer**：カーネル＆venv両対応＋**フォールバック**（未導入時は通常DL）\n",
    "- **Audio(type=\"numpy\")**（UIに確実表示）＋ **Debug Log**（詳細ログ）＋ テストトーン**フォールバック**\n",
    "- **Whisper** の **task/language 明示**（挙動固定）\n",
    "- **CUDA 12.x 自動整合**（nvJitLink/cusparse符号ずれに対応）\n",
    "- **share=True**（Ports不要）\n",
    "- **LOCAL_ONLY_MODE=1** で **ローカルモデル強制**が既定（必要なら 0 に）\n",
    "- **EOS即終了対策**：`min_new_tokens` 導入＋**Ignore EOS** トグル＋**PADトークン自動追加**\n",
    "\n",
    "**実行順序：0 → 1 → 2 → 3 → 4 → 4.5 → 4.9 → 5 → 5.1 → 6 → 7**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6bb8b82",
   "metadata": {},
   "source": [
    "## 0️⃣ Matplotlib backend を Agg 固定（バックエンドエラー回避）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789f2c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, matplotlib\n",
    "os.environ[\"MPLBACKEND\"] = \"Agg\"\n",
    "matplotlib.use(\"Agg\")\n",
    "print(\"matplotlib backend:\", matplotlib.get_backend())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef71602",
   "metadata": {},
   "source": [
    "## 1️⃣ /tmp 構成＆環境変数（/dev/shmは使いません）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ef29bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "base = Path(\"/tmp/llasa_space\")\n",
    "for p in [\"hf\", \".cache\", \"tmp\", \"outputs\", \"app\"]:\n",
    "    (base / p).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "os.environ[\"HF_HOME\"]=\"/tmp/llasa_space/hf\"\n",
    "os.environ[\"HUGGINGFACE_HUB_CACHE\"]=\"/tmp/llasa_space/hf\"\n",
    "os.environ[\"TRANSFORMERS_CACHE\"]=\"/tmp/llasa_space/hf\"\n",
    "os.environ[\"XDG_CACHE_HOME\"]=\"/tmp/llasa_space/.cache\"\n",
    "os.environ[\"TMPDIR\"]=\"/tmp/llasa_space/tmp\"\n",
    "os.environ[\"LLASA_OUT\"]=\"/tmp/llasa_space/outputs\"\n",
    "# 高速DL（Step 4.9 と 4 で kernel/venv 双方対応）\n",
    "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"]=\"1\"\n",
    "# ローカルモデルの強制（必要なら \"0\" に変更）\n",
    "os.environ[\"LOCAL_ONLY_MODE\"]=\"1\"\n",
    "\n",
    "print({k: os.environ[k] for k in [\"HF_HOME\",\"HUGGINGFACE_HUB_CACHE\",\"TRANSFORMERS_CACHE\",\"XDG_CACHE_HOME\",\"TMPDIR\",\"LLASA_OUT\",\"HF_HUB_ENABLE_HF_TRANSFER\",\"LOCAL_ONLY_MODE\"]})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52aa0530",
   "metadata": {},
   "source": [
    "## 2️⃣ GPU / /tmp 空き容量チェック"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a509c20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "try:\n",
    "    import torch\n",
    "    print(\"CUDA available:\", torch.cuda.is_available())\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "except Exception as e:\n",
    "    print(\"torch not installed yet:\", e)\n",
    "\n",
    "import shutil\n",
    "def human(n):\n",
    "    for u in ['B','KB','MB','GB','TB']:\n",
    "        if n<1024: return f\"{n:.1f}{u}\"\n",
    "        n/=1024\n",
    "    return f\"{n:.1f}PB\"\n",
    "total, used, free = shutil.disk_usage(\"/tmp\")\n",
    "print(\"/tmp free:\", human(free), \"total:\", human(total))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8289f878",
   "metadata": {},
   "source": [
    "## 3️⃣ OS依存（ffmpeg, git）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71df1293",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!sudo apt-get update -y\n",
    "!sudo apt-get install -y ffmpeg git\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb61df6",
   "metadata": {},
   "source": [
    "## 4️⃣ venv作成＆依存導入（torch 2.4.1 + cu121）＋ **venv側に hf_transfer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253cdcb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!python3 -m venv /tmp/llasa_space/app/venv\n",
    "VENV_PIP=\"/tmp/llasa_space/app/venv/bin/pip\"\n",
    "VENV_PY=\"/tmp/llasa_space/app/venv/bin/python\"\n",
    "\n",
    "!$VENV_PIP uninstall -y torch torchvision torchaudio || true\n",
    "!$VENV_PIP install -U pip wheel setuptools\n",
    "\n",
    "# PyTorch 2.4.1 + cu121（実績構成）\n",
    "!$VENV_PIP install --index-url https://download.pytorch.org/whl/cu121 \\\n",
    "  torch==2.4.1 torchvision==0.19.1 torchaudio==2.4.1\n",
    "\n",
    "# ランタイム依存\n",
    "!$VENV_PIP install \"transformers>=4.43.0\" accelerate gradio==4.44.0 soundfile numpy scipy huggingface_hub==0.24.6\n",
    "# codec（Spaceは 0.1.3 を想定）\n",
    "!$VENV_PIP install xcodec2==0.1.3\n",
    "# venv 側にも hf_transfer を導入（実行時DLで必要になるため）\n",
    "!$VENV_PIP install -U hf_transfer\n",
    "\n",
    "print(\"Base installs (incl. hf_transfer in venv) completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad5dd42",
   "metadata": {},
   "source": [
    "## 4.5️⃣ CUDA 12.x マイナー不整合の自動整合（nvJitLink/cusparse 等）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30f0f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, re, subprocess\n",
    "\n",
    "VENV_PIP=\"/tmp/llasa_space/app/venv/bin/pip\"\n",
    "VENV_PY =\"/tmp/llasa_space/app/venv/bin/python\"\n",
    "\n",
    "def set_ld_library_path():\n",
    "    site=\"/tmp/llasa_space/app/venv/lib/python3.11/site-packages/nvidia\"\n",
    "    paths=[]\n",
    "    for sub in [\"cuda_runtime\",\"cublas\",\"cusparse\",\"cudnn\",\"nvjitlink\",\"cuda_nvrtc\"]:\n",
    "        p=os.path.join(site, sub, \"lib\")\n",
    "        if os.path.isdir(p): paths.append(p)\n",
    "    ld=\":\".join(paths)\n",
    "    if \"LD_LIBRARY_PATH\" in os.environ and os.environ[\"LD_LIBRARY_PATH\"]:\n",
    "        ld = ld + \":\" + os.environ[\"LD_LIBRARY_PATH\"]\n",
    "    os.environ[\"LD_LIBRARY_PATH\"]=ld\n",
    "    return ld\n",
    "\n",
    "def install_cuda_minor(minor:str):\n",
    "    pkgs=[\n",
    "        f\"nvidia-cuda-nvrtc-cu12==12.{minor}.*\",\n",
    "        f\"nvidia-cuda-runtime-cu12==12.{minor}.*\",\n",
    "        f\"nvidia-cublas-cu12==12.{minor}.*\",\n",
    "        f\"nvidia-cusparse-cu12==12.{minor}.*\",\n",
    "        f\"nvidia-nvjitlink-cu12==12.{minor}.*\",\n",
    "        \"nvidia-cudnn-cu12>=9.0.0\",\n",
    "    ]\n",
    "    cmd=[VENV_PIP,\"install\",\"-U\"]+pkgs\n",
    "    print(\"Installing CUDA libs for 12.%s.* ...\" % minor)\n",
    "    subprocess.run(cmd, check=False)\n",
    "\n",
    "def try_import():\n",
    "    code = \"import os; print('LD=',os.environ.get('LD_LIBRARY_PATH','')[:200]); import torch, torchaudio; print('OK', torch.__version__, torch.version.cuda, torch.cuda.is_available(), torchaudio.__version__)\"\n",
    "    r = subprocess.run([VENV_PY,\"-c\",code], capture_output=True, text=True)\n",
    "    return r.returncode, r.stdout + r.stderr\n",
    "\n",
    "def parse_needed_minor(msg:str):\n",
    "    m = re.search(r\"__nvJitLink(?:AddData|Complete)_(12)_(\\d+)\", msg)\n",
    "    if m:\n",
    "        return m.group(2)\n",
    "    return None\n",
    "\n",
    "set_ld_library_path()\n",
    "rc, out = try_import()\n",
    "print(out)\n",
    "if rc==0:\n",
    "    print(\"✔ CUDA libs already consistent.\")\n",
    "else:\n",
    "    need = parse_needed_minor(out)\n",
    "    tried=set()\n",
    "    candidates = ([need] if need else []) + [\"5\",\"4\",\"3\",\"2\",\"1\"]\n",
    "    for minor in candidates:\n",
    "        if minor in tried or minor is None: continue\n",
    "        tried.add(minor)\n",
    "        install_cuda_minor(minor)\n",
    "        set_ld_library_path()\n",
    "        rc, out = try_import()\n",
    "        print(out)\n",
    "        if rc==0:\n",
    "            print(f\"✔ Fixed by installing cu12.{minor} libs.\")\n",
    "            break\n",
    "    if rc!=0:\n",
    "        print(\"❌ Import still failing. Please copy the above logs and share.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162d6556",
   "metadata": {},
   "source": [
    "## 4.9️⃣ カーネル側に `hf_transfer` を自動導入（高速DLの有効化）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e141ac7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, sys, subprocess\n",
    "if os.environ.get(\"HF_HUB_ENABLE_HF_TRANSFER\",\"0\") == \"1\":\n",
    "    try:\n",
    "        import hf_transfer  # noqa: F401\n",
    "        print(\"hf_transfer already available (kernel).\")\n",
    "    except Exception:\n",
    "        print(\"Installing hf_transfer into the **kernel** Python...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"hf_transfer\"])\n",
    "        import hf_transfer  # noqa: F401\n",
    "        print(\"hf_transfer installed (kernel).\")\n",
    "else:\n",
    "    print(\"HF_HUB_ENABLE_HF_TRANSFER is not 1; skipping.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db93623a",
   "metadata": {},
   "source": [
    "## 5️⃣ モデルDL（Anime-Llasa-3B / Anime-XCodec2 / Whisper Turbo を /tmp に）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5539efbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from huggingface_hub import snapshot_download\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "BASE=\"/tmp/llasa_space/hf/models\"\n",
    "Path(BASE).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "targets={\n",
    "    \"Anime-Llasa-3B\":\"NandemoGHS/Anime-Llasa-3B\",\n",
    "    \"Anime-XCodec2\":\"NandemoGHS/Anime-XCodec2\",\n",
    "    \"Whisper-Large-V3-Turbo\":\"openai/whisper-large-v3-turbo\"\n",
    "}\n",
    "for name, repo in targets.items():\n",
    "    out=f\"{BASE}/{name}\"\n",
    "    if not os.path.isdir(out):\n",
    "        print(\"Downloading\", repo, \"->\", out)\n",
    "        snapshot_download(repo_id=repo, local_dir=out, local_dir_use_symlinks=False,\n",
    "                          resume_download=True, max_workers=8)\n",
    "    else:\n",
    "        print(\"Exists:\", out)\n",
    "print(\"Models ready.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2555080e",
   "metadata": {},
   "source": [
    "## 5.1️⃣ モデル配置の検証（必要ファイルの有無チェック）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebf446e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "\n",
    "BASE=\"/tmp/llasa_space/hf/models\"\n",
    "paths = {\n",
    "    \"LLASA\": f\"{BASE}/Anime-Llasa-3B\",\n",
    "    \"XCODEC2\": f\"{BASE}/Anime-XCodec2\",\n",
    "    \"WHISPER\": f\"{BASE}/Whisper-Large-V3-Turbo\",\n",
    "}\n",
    "\n",
    "def must(p, names):\n",
    "    from pathlib import Path\n",
    "    missing=[n for n in names if not Path(p, n).exists()]\n",
    "    return missing\n",
    "\n",
    "must_llasa = [\"config.json\", \"generation_config.json\", \"tokenizer_config.json\"]\n",
    "must_xcodec = [\"config.json\"]\n",
    "must_whisper = [\"config.json\", \"preprocessor_config.json\"]\n",
    "\n",
    "print(\"LLASA missing:\", must(paths[\"LLASA\"], must_llasa))\n",
    "print(\"XCODEC2 missing:\", must(paths[\"XCODEC2\"], must_xcodec))\n",
    "print(\"WHISPER missing:\", must(paths[\"WHISPER\"], must_whisper))\n",
    "\n",
    "for k,p in paths.items():\n",
    "    p=Path(p)\n",
    "    print(f\"{k}: {p} exists={p.exists()} files={len(list(p.glob('*')))}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a567abeb",
   "metadata": {},
   "source": [
    "## 6️⃣ Gradio アプリ配置（v6：min_new_tokens＋Ignore EOS＋PAD追加）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d980456",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "app_code = r'''\n",
    "import os, re, time\n",
    "# ---- Guard: fallback if hf_transfer missing in runtime (venv) ----\n",
    "try:\n",
    "    import hf_transfer  # noqa: F401\n",
    "except Exception:\n",
    "    os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"0\"\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "import numpy as np\n",
    "import torch, torchaudio, soundfile as sf\n",
    "import gradio as gr\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from xcodec2.modeling_xcodec2 import XCodec2Model\n",
    "\n",
    "os.environ.setdefault(\"MPLBACKEND\",\"Agg\")\n",
    "\n",
    "HF_BASE=\"/tmp/llasa_space/hf/models\"\n",
    "LLASA_DIR=f\"{HF_BASE}/Anime-Llasa-3B\"\n",
    "XCODEC_DIR=f\"{HF_BASE}/Anime-XCodec2\"\n",
    "WHISPER_DIR=f\"{HF_BASE}/Whisper-Large-V3-Turbo\"\n",
    "OUT_DIR=os.environ.get(\"LLASA_OUT\",\"/tmp/llasa_space/outputs\")\n",
    "LOCAL_ONLY = os.environ.get(\"LOCAL_ONLY_MODE\",\"1\") == \"1\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "DEVICE=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"[Device] {DEVICE}  bf16={torch.cuda.is_available() and torch.cuda.is_bf16_supported()}\")\n",
    "DTYPE = torch.bfloat16 if (DEVICE==\"cuda\" and torch.cuda.is_bf16_supported()) else torch.float16\n",
    "\n",
    "def _test_tone(seconds=1.0, sr=16000, freq=1000.0):\n",
    "    t = np.linspace(0, seconds, int(sr*seconds), endpoint=False, dtype=np.float32)\n",
    "    y = (np.sin(2*np.pi*freq*t)).astype(\"float32\")\n",
    "    return (sr, y)\n",
    "\n",
    "def _npinfo(x):\n",
    "    if isinstance(x, np.ndarray):\n",
    "        try:\n",
    "            return f\"ndarray shape={x.shape} dtype={x.dtype} min={x.min():.4f} max={x.max():.4f}\"\n",
    "        except Exception:\n",
    "            return f\"ndarray shape={x.shape} dtype={x.dtype}\"\n",
    "    return str(type(x))\n",
    "\n",
    "def _path_or_id(local_dir, repo_id):\n",
    "    return local_dir if (os.path.isdir(local_dir) and LOCAL_ONLY) else (local_dir if os.path.isdir(local_dir) else repo_id)\n",
    "\n",
    "# Loaders (Space準拠) — LOCAL_ONLY_MODE=1 ならローカルを強制\n",
    "llasa_src = _path_or_id(LLASA_DIR, \"NandemoGHS/Anime-Llasa-3B\")\n",
    "xcodec_src = _path_or_id(XCODEC_DIR, \"NandemoGHS/Anime-XCodec2\")\n",
    "whisp_src  = _path_or_id(WHISPER_DIR, \"openai/whisper-large-v3-turbo\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(llasa_src, local_files_only=LOCAL_ONLY)\n",
    "print(\"[LLASA Tokenizer]\", llasa_src)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    llasa_src, trust_remote_code=True, torch_dtype=DTYPE, local_files_only=LOCAL_ONLY\n",
    ").to(DEVICE).eval()\n",
    "print(\"[LLASA Model]\", llasa_src)\n",
    "\n",
    "# PADが無ければ新規追加（モデル埋め込みもresize）\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "pad_id_force = tokenizer.pad_token_id\n",
    "\n",
    "codec_model = XCodec2Model.from_pretrained(xcodec_src, local_files_only=LOCAL_ONLY).to(DEVICE).eval()\n",
    "print(\"[XCodec2 Model]\", xcodec_src)\n",
    "\n",
    "# Whisper pipeline（generate_kwargs は呼び出し時に渡す）\n",
    "whisper_pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=whisp_src,\n",
    "    torch_dtype=torch.float16 if DEVICE==\"cuda\" else torch.float32,\n",
    "    device=0 if DEVICE==\"cuda\" else -1\n",
    ")\n",
    "print(\"[Whisper Model]\", whisp_src)\n",
    "\n",
    "INVALID_PATTERN = re.compile(r\"[^\\u3040-\\u309F\\u30A0-\\u30FF\\u4E00-\\u9FFF\\u3400-\\u4DBF\\u3005\\u0041-\\u005A\\u0061-\\u007A\\u0030-\\u0039。、!?…♪♡○]\")\n",
    "def normalize(text: str) -> str:\n",
    "    text = re.sub(r\"\\t\", \"\", text)\n",
    "    text = re.sub(r\"[\\n]\", \"\", text)\n",
    "    text = text.replace(\" \", \"\")\n",
    "    text = re.sub(r\"[;▼♀♂《》≪≫①②③④⑤⑥]\", \"\", text)\n",
    "    text = re.sub(r\"[\\u02d7\\u2010-\\u2015\\u2043\\u2212\\u23af\\u23e4\\u2500\\u2501\\u2e3a\\u2e3b]\", \"\", text)\n",
    "    text = text.replace(\"？\",\"?\").replace(\"！\",\"!\").replace(\"♥\",\"♡\").replace(\"●\",\"○\").replace(\"◯\",\"○\").replace(\"〇\",\"○\")\n",
    "    text = re.sub(r\"…{3,}\", \"……\", text)\n",
    "    text = INVALID_PATTERN.sub(\"\", text)\n",
    "    return text\n",
    "\n",
    "def to_16k_mono(wav: torch.Tensor, sr: int) -> torch.Tensor:\n",
    "    if wav.dim()==1:\n",
    "        wav=wav.unsqueeze(0)\n",
    "    if wav.size(0)>1:\n",
    "        wav = wav.mean(0, keepdim=True)\n",
    "    if sr!=16000:\n",
    "        wav = torchaudio.functional.resample(wav, sr, 16000)\n",
    "    return wav  # [1, T]\n",
    "\n",
    "def ids_to_speech_tokens(ids):\n",
    "    return [f\"<|s_{int(i)}|>\" for i in ids]\n",
    "\n",
    "def extract_speech_ids(token_strings):\n",
    "    speech_ids = []\n",
    "    for s in token_strings:\n",
    "        if s.startswith(\"<|s_\") and s.endswith(\"|>\"):\n",
    "            try: speech_ids.append(int(s[4:-2]))\n",
    "            except: pass\n",
    "    return speech_ids\n",
    "\n",
    "def ensure_pad_and_mask(input_ids, tok, pad_id_hint=None):\n",
    "    eos_id = tok.convert_tokens_to_ids(\"<|SPEECH_GENERATION_END|>\")\n",
    "    pad_id = tok.pad_token_id if tok.pad_token_id is not None else (pad_id_hint if pad_id_hint is not None else eos_id)\n",
    "    attn   = (input_ids != pad_id).long()\n",
    "    return pad_id, eos_id, attn\n",
    "\n",
    "def infer(sample_audio_path, target_text, temperature, top_p, repetition_penalty, asr_mode, min_new_tokens, ignore_eos, progress=gr.Progress()):\n",
    "    log_lines=[]\n",
    "    import time as _time\n",
    "    _t0=_time.time()\n",
    "    def _log(s): log_lines.append(str(s))\n",
    "\n",
    "    if not target_text or not target_text.strip():\n",
    "        gr.Warning(\"テキストを入力してください。\")\n",
    "        return (16000, np.zeros(1600, dtype=np.float32)), \"no text\"\n",
    "\n",
    "    if len(target_text) > 300:\n",
    "        gr.Warning(\"テキストが長すぎます（300字以内推奨）。先頭300字で生成します。\")\n",
    "        target_text = target_text[:300]\n",
    "\n",
    "    target_text = normalize(target_text)\n",
    "    _log(f\"text(len={len(target_text)})='{target_text[:120]}'\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        if sample_audio_path:\n",
    "            progress(0, \"参照音声をロード中...\")\n",
    "            wav, sr = torchaudio.load(sample_audio_path)\n",
    "            if wav.shape[1] / sr > 15:\n",
    "                wav = wav[:, : sr * 15]\n",
    "            prompt_wav = to_16k_mono(wav, sr)  # [1, T] @16k\n",
    "            prompt_len = prompt_wav.shape[1]\n",
    "\n",
    "            progress(0.4, \"Whisperで文字起こし中...\")\n",
    "            if asr_mode == \"auto_transcribe_ja\":\n",
    "                asr = whisper_pipe(prompt_wav[0].numpy(), return_timestamps=False,\n",
    "                                   generate_kwargs={\"task\":\"transcribe\",\"language\":\"ja\"})\n",
    "            elif asr_mode == \"translate_en\":\n",
    "                asr = whisper_pipe(prompt_wav[0].numpy(), return_timestamps=False,\n",
    "                                   generate_kwargs={\"task\":\"translate\",\"language\":\"en\"})\n",
    "            else:  # auto_transcribe\n",
    "                asr = whisper_pipe(prompt_wav[0].numpy(), return_timestamps=False,\n",
    "                                   generate_kwargs={\"task\":\"transcribe\"})\n",
    "            prompt_text = asr[\"text\"].strip()\n",
    "            _log(f\"ASR: len={len(prompt_text)} text='{prompt_text[:120]}'\")\n",
    "\n",
    "            progress(0.6, \"参照音声をコード化中...\")\n",
    "            vq = codec_model.encode_code(input_waveform=prompt_wav.to(DEVICE))[0,0,:]\n",
    "            _log(f\"VQ prompt codes: {int(vq.numel())}\")\n",
    "            speech_prefix = \"\".join(ids_to_speech_tokens(vq))\n",
    "            input_text = (prompt_text + \" \" + target_text).strip()\n",
    "            assistant_content = \"<|SPEECH_GENERATION_START|>\" + speech_prefix\n",
    "        else:\n",
    "            prompt_len = 0\n",
    "            input_text = target_text\n",
    "            assistant_content = \"<|SPEECH_GENERATION_START|>\"\n",
    "\n",
    "        progress(0.8, \"音声トークンを生成中...\")\n",
    "        formatted = f\"<|TEXT_UNDERSTANDING_START|>{input_text}<|TEXT_UNDERSTANDING_END|>\"\n",
    "        chat = [\n",
    "            {\"role\":\"user\",\"content\":\"Convert the text to speech:\" + formatted},\n",
    "            {\"role\":\"assistant\",\"content\": assistant_content},\n",
    "        ]\n",
    "        input_ids = tokenizer.apply_chat_template(chat, tokenize=True, return_tensors=\"pt\",\n",
    "                                                  continue_final_message=True).to(DEVICE)\n",
    "\n",
    "        pad_id, eos_id, attn = ensure_pad_and_mask(input_ids, tokenizer, pad_id_hint=pad_id_force)\n",
    "        _log(f\"ids_in={int(input_ids.numel())} eos={eos_id} pad={pad_id} min_new_tokens={int(min_new_tokens)} ignore_eos={bool(ignore_eos)}\")\n",
    "\n",
    "        _log(\"generate() starting...\")\n",
    "        outputs = model.generate(\n",
    "            input_ids,\n",
    "            max_length=2048,\n",
    "            min_new_tokens=int(min_new_tokens),\n",
    "            eos_token_id=None if ignore_eos else eos_id,\n",
    "            pad_token_id=pad_id,\n",
    "            attention_mask=attn,\n",
    "            do_sample=True,\n",
    "            top_p=float(top_p),\n",
    "            temperature=float(temperature),\n",
    "            repetition_penalty=float(repetition_penalty),\n",
    "        )\n",
    "\n",
    "        gen_ids = outputs[0][input_ids.shape[1]:]\n",
    "        speech_tokens_str = tokenizer.batch_decode(gen_ids, skip_special_tokens=True)\n",
    "        _log(f\"gen_ids={int(gen_ids.numel())} decoded_strings={len(speech_tokens_str)}\")\n",
    "        speech_ids = extract_speech_ids(speech_tokens_str)\n",
    "        _log(f\"speech_ids extracted={len(speech_ids)}\")\n",
    "        if not speech_ids:\n",
    "            _log(\"[fallback] no speech_ids -> test tone\")\n",
    "            t = np.linspace(0, 1.0, 16000, endpoint=False, dtype=np.float32)\n",
    "            y = (np.sin(2*np.pi*880*t)).astype(\"float32\")\n",
    "            return (16000, y), \"\\\\n\".join(log_lines)\n",
    "\n",
    "        codes = torch.tensor(speech_ids, device=DEVICE).view(1,1,-1)\n",
    "        gen_wav = codec_model.decode_code(codes)  # [1,1,T]\n",
    "\n",
    "        if sample_audio_path and prompt_len>0:\n",
    "            if gen_wav.shape[-1] > prompt_len:\n",
    "                gen_wav = gen_wav[:, :, prompt_len:]\n",
    "                _log(f\"trimmed leading {prompt_len} samples (prompt)\")\n",
    "            else:\n",
    "                _log(f\"skip trim: gen_len={gen_wav.shape[-1]} <= prompt_len={prompt_len}\")\n",
    "\n",
    "        arr = gen_wav[0,0,:].float().cpu().numpy()\n",
    "        ts = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "        out = os.path.join(OUT_DIR, f\"llasa_space_{ts}.wav\")\n",
    "        sf.write(out, arr, 16000)\n",
    "        _log(f\"wav: shape={arr.shape} min={arr.min():.4f} max={arr.max():.4f} file={out}\")\n",
    "        _log(f\"total_time={time.time()-_t0:.2f}s\")\n",
    "\n",
    "        # Validate audio and fallback if needed\n",
    "        if not isinstance(arr, np.ndarray) or arr.size < 320 or not np.isfinite(arr).all():\n",
    "            _log(\"[fallback] returning test tone (invalid or too short audio)\")\n",
    "            t = np.linspace(0, 1.0, 16000, endpoint=False, dtype=np.float32)\n",
    "            y = (np.sin(2*np.pi*880*t)).astype(\"float32\")\n",
    "            return (16000, y), \"\\\\n\".join(log_lines)\n",
    "\n",
    "        return (16000, arr), \"\\\\n\".join(log_lines)\n",
    "\n",
    "with gr.Blocks(title=\"Anime-Llasa-3B Space-Style\", theme=gr.themes.Base()) as app:\n",
    "    gr.Markdown(\"### Anime-Llasa-3B — HF Space 構成（/tmp 完結）\")\n",
    "    ref = gr.Audio(label=\"Reference Audio (optional, <=15s)\", type=\"filepath\")\n",
    "    text= gr.Textbox(label=\"Text\", lines=8, placeholder=\"テキストを入力\")\n",
    "    with gr.Row():\n",
    "        temperature        = gr.Slider(0.0,1.2,1.0,0.05,label=\"Temperature\")\n",
    "        top_p              = gr.Slider(0.5,1.0,0.95,0.01,label=\"Top-p\")\n",
    "        repetition_penalty = gr.Slider(1.0,1.5,1.05,0.05,label=\"Repetition Penalty\")\n",
    "    with gr.Row():\n",
    "        asr_mode = gr.Dropdown(\n",
    "            [\"auto_transcribe_ja\", \"auto_transcribe\", \"translate_en\"],\n",
    "            value=\"auto_transcribe_ja\",\n",
    "            label=\"ASRモード\"\n",
    "        )\n",
    "        min_new_tokens = gr.Slider(10, 200, 50, 1, label=\"min_new_tokens\")\n",
    "        ignore_eos     = gr.Checkbox(value=False, label=\"Ignore EOS (debug)\")\n",
    "    go   = gr.Button(\"Synthesize\", variant=\"primary\")\n",
    "    aout = gr.Audio(label=\"Output\", type=\"numpy\", autoplay=True, show_download_button=True)\n",
    "    log  = gr.Textbox(label=\"Debug Log\", lines=14, value=\"\", interactive=False)\n",
    "\n",
    "    go.click(infer, [ref,text,temperature,top_p,repetition_penalty,asr_mode,min_new_tokens,ignore_eos], [aout, log])\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    site=\"/tmp/llasa_space/app/venv/lib/python3.11/site-packages/nvidia\"\n",
    "    os.environ[\"LD_LIBRARY_PATH\"]=\":\".join(\n",
    "        p for s in [\"cuda_runtime\",\"cublas\",\"cusparse\",\"cudnn\",\"nvjitlink\",\"cuda_nvrtc\"]\n",
    "        if os.path.isdir((p:=f\"{site}/{s}/lib\"))\n",
    "    )\n",
    "    app.queue().launch(server_name=\"0.0.0.0\", server_port=7860, share=True, show_error=True)\n",
    "'''\n",
    "open(\"/tmp/llasa_space/app/app.py\",\"w\").write(app_code)\n",
    "print(\"Wrote /tmp/llasa_space/app/app.py\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c885d2f",
   "metadata": {},
   "source": [
    "## 7️⃣ 起動（ログの `https://xxxx.gradio.live` を開く）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c514a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!MPLBACKEND=Agg /tmp/llasa_space/app/venv/bin/python /tmp/llasa_space/app/app.py\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}